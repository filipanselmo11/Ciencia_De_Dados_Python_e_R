{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rede_Neural():\n",
    "    \n",
    "    ##Inicializando os elementos da rede neural\n",
    "    def __init__(self, n_entradas = 2, n_camadas = 3, n_saidas = 1):\n",
    "        \n",
    "        ##Arquitetura da rede neural\n",
    "        self.n_entradas = n_entradas\n",
    "        self.n_camadas = n_camadas\n",
    "        self.n_saidas = n_saidas\n",
    "        \n",
    "        ## Inicialização das matrizes de pesos\n",
    "        ## Distribuição U(-1, 1)\n",
    "        self. w_1 = np.random.uniform(low = -1, high = 1, size = (self.n_camadas, self.n_entradas))\n",
    "        self. w_2 = np.random.uniform(low = -1, high = 1, size = (self.n_saidas, self.n_camadas))\n",
    "        \n",
    "        ## Acrescimo do vies(bias)\n",
    "        ## Vies é aleatório com distribuição U(-0.5,0.5)\n",
    "        self.bias_h = np.random.rand(self.n_camadas, 1) - 0.5\n",
    "        self.bias_o = np.random.rand(self.n_saidas, 1) - 0.5\n",
    "        \n",
    "        ##Função de ativação Sigmoide\n",
    "        self.sig = lambda x: 1/(1+np.exp(-x))\n",
    "        \n",
    "        ## Derivada da função sigmoide\n",
    "        self.dsig = lambda y: y * (1.0 - y)\n",
    "        \n",
    "        ## Guardando o erro ao longo do treinamento\n",
    "        self.erros = []\n",
    "        \n",
    "    ## Algoritmo de backpropragation de aprendizado supervisionado para redes neurais multi-layer perceptron\n",
    "    def fit(self, x_treinamento, y_alvos, taxa_aprendizado = 0.1, epocas = 1000, tol = 0):\n",
    "        \n",
    "        ##Barra de Progresso\n",
    "        barra = IntProgress(min=0, max=epocas)\n",
    "        print(\"Progresso do treinamento: \")\n",
    "        display(barra)\n",
    "        \n",
    "        #Treinamento enquanto h não da erro ?\n",
    "        haErro = True\n",
    "        i = 0\n",
    "        \n",
    "        while(i < epocas) and haErro:\n",
    "            for x_tr, y_alvo in zip(x_treinamento, y_alvos):\n",
    "                \n",
    "                ## Ajustando a dimensao de entrada\n",
    "                x = np.array(x_tr, ndim=2).T\n",
    "                \n",
    "                ### Etapa forwad\n",
    "                # Entrada --> Camada Oculta\n",
    "                saida_oculta = self.sig(np.add((np.dot(self.w_1, x)), self.bias_h))\n",
    "                # Camada oculta --> Camada de saida\n",
    "                y_previsto = self.sig(np.add((np.dot(self.w_2, saida_oculta)), self.bias_o))\n",
    "                \n",
    "                ## Calculo do erro a cada época\n",
    "                erro = y_alvo - y_previsto\n",
    "                \n",
    "                ### Etapa backwards\n",
    "                \n",
    "                ### Calculo do erro na camada de saida\n",
    "                erro_oculto = np.dot(self.w_2.T, erro)\n",
    "                ## Ajuste dos pesos na camada de saida, no sentindo oposto ao gradiente descendente\n",
    "                ## Observe o delta w\n",
    "                self.w_2 += taxa_aprendizado*(np.dot((erro * (self.dsig(y_previsto))), np.transpose(saida_oculta)))\n",
    "                \n",
    "                ## Ajustando o vies da camada de saida\n",
    "                self.bias_o += taxa_aprendizado*(erro*(self.dsig(y_previsto)))\n",
    "                \n",
    "                ### Considera apenas uma camada oculta\n",
    "                ## Calculo do erro na camada oculta\n",
    "                self.w_1 += taxa_aprendizado*(np.dot((erro_oculto*(self.dsig(saida_oculta))), np.transpose(x)))\n",
    "                \n",
    "                #Ajustando o vies na camada oculta\n",
    "                self.bias_h += taxa_aprendizado*(erro_oculto*(self.dsig(saida_oculta)))\n",
    "                \n",
    "            ## Erro medio quadratico a cada epoca\n",
    "            erroEpoca = mean_squared_error(y_alvo, y_previsto)\n",
    "            self.erros.append(erroEpoca)\n",
    "            \n",
    "            \n",
    "            ## Atualizada a barra de progresso\n",
    "            barra.value += 1\n",
    "            \n",
    "            if(erroEpoca <= tol):\n",
    "                haErro = False\n",
    "            i += 1\n",
    "## Reaiza a etapa forward da rede e aplica a funcao degrau com theta = 0.5 para determinar a classe\n",
    "\n",
    "    def predict(self, x_con):\n",
    "        #Converter as entradas para a matriz 2d.\n",
    "        x = np.array(x_con, ndmin=2).T\n",
    "        \n",
    "        # alculando a saida dos neuronios da camada oculta\n",
    "        saida_oculta = self.sig(np.add((np.dot(self.w_1, x)), self.bias_h))\n",
    "        # Calculando a saida final da rede neural\n",
    "        saida_final = self.sig(np.add((np.dot(self.w_2, saida_oculta)), self.bias_o))\n",
    "        \n",
    "        return [1 if x >= 0.5 else 0 for x in saida_final]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
